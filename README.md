# ğŸ”¢ QMNIST Digit Classification using Multi-Layer Perceptron (PyTorch)

This project explores digit classification on the **QMNIST dataset** using a Multi-Layer Perceptron (MLP). It includes robust experimentation with model architecture, loss functions, activation layers, optimizers, and dropout to optimize accuracy.

---


## ğŸ§  Dataset: QMNIST

* **QMNIST** is an extended version of the classic MNIST handwritten digits dataset.
* 28x28 grayscale images of digits (0â€“9)
* Comes with training and test sets pre-split via `torchvision.datasets.QMNIST`

---

## ğŸ§ª Objective

> Build, train, and optimize a multi-layer perceptron (MLP) to classify digits in the QMNIST dataset. Analyze the impact of different model hyperparameters.

---

## ğŸ”§ Model Architecture

### ğŸ”¹ Baseline MLP

```
Input: 784 (28x28 flattened)
â†’ Dense(128) + ReLU
â†’ Dense(64) + ReLU
â†’ Dense(10) (logits)
```

* **Loss**: CrossEntropyLoss
* **Optimizer**: Adam
* **Batch size**: 64
* **Epochs**: 5

### ğŸ”¹ Modified MLP (increased capacity)

```
Input: 784
â†’ Dense(256) + ReLU
â†’ Dense(64) + ReLU
â†’ Dense(10)
```

---

## ğŸ“Š Performance

| Model Version                                   | Test Accuracy |
| ----------------------------------------------- | ------------- |
| Baseline MLP                                    | 96.43%        |
| With 256 hidden units                           | 97.24% âœ…      |
| Best Hyperparameter Run (SGD, ReLU, No Dropout) | **96.70%**    |
| Worst Hyperparameter Run (SGD, Sigmoid)         | 92.77%        |

---

## ğŸ”¬ Hyperparameter Experiments

| Config                                            | Accuracy                   |
| ------------------------------------------------- | -------------------------- |
| **Optimizer**: SGD                                | âœ… Best Performance         |
| **Loss**: CrossEntropy > NLLLoss                  | âœ… Better with raw logits   |
| **Activation**: ReLU > LeakyReLU > Tanh > Sigmoid | âœ… ReLU consistently better |
| **Dropout**: 0.0 (no dropout)                     | âœ… Highest performance      |

> ğŸ§  ReLU activation with SGD and no dropout gave the best test accuracy (96.70%).

---

## ğŸ“‰ Training Visualization

* Training loss printed every 100 batches
* Accuracy measured on train/test
* Plots included for accuracy and predictions

---

## ğŸ§ª Sample Predictions

* Predicted vs actual visualizations
* Matplotlib-based plot to display digit and its predicted label

---

## ğŸ“Œ How to Run
pip install torch torchvision matplotlib
Run all cells in `qmnist_classification.ipynb`

---

## ğŸ“˜ Key Learnings

* **Neural capacity matters**: Increasing neurons improves performance
* **SGD outperformed Adam**: With right tuning, classic optimizers still shine
* **Dropout isnâ€™t always needed**: Especially for small, clean datasets like MNIST/QMNIST
* **ReLU is generally best**: Simple and effective activation

---

## ğŸ Conclusion

This project showcases how architectural changes and careful hyperparameter tuning can significantly improve digit classification performance using a simple MLP on QMNIST.

---
