{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92db810-62b8-4f22-bc25-0ddb07e05441",
   "metadata": {},
   "source": [
    "### Assignment 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434c517-217e-4b65-9085-1c6f7b46c636",
   "metadata": {},
   "source": [
    "Step 2. In a new .ipynb notebook, reproduce the results utilizing the \"QMNIST\" dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3590d1e4-d369-49c8-a4fa-b3c65db0437b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b773a2f7-ce6f-4b9e-ae19-8e95b6503fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Load the QMNIST dataset using torchvision\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the QMNIST training and test datasets\n",
    "train_data = datasets.QMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.QMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54928b89-900b-4836-a93b-37e112f9788a",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fff28d6-f8ea-4808-8d5d-3ab0856719b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the label map (0-9 are digits, so no need for custom names)\n",
    "labels_map = {i: str(i) for i in range(10)}\n",
    "\n",
    "# Create the figure\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Plot random images from the training dataset\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # Randomly pick an image from the train dataset\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    \n",
    "    # Add a subplot in the grid\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    \n",
    "    # Set the title and plot the image\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d246ec91-1021-4f5e-ace5-60b7ee6bafe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the label map (0-9 are digits, so no need for custom names)\n",
    "labels_map = {i: str(i) for i in range(10)}\n",
    "\n",
    "# Create the figure\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "# Plot random images from the test dataset\n",
    "for i in range(1, cols * rows + 1):\n",
    "    # Randomly pick an image from the test dataset\n",
    "    sample_idx = torch.randint(len(test_data), size=(1,)).item()  # Use 'test_data' for the test dataset\n",
    "    img, label = test_data[sample_idx]\n",
    "    \n",
    "    # Add a subplot in the grid\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    \n",
    "    # Set the title and plot the image\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1d321e-19ed-4a5b-80af-16085ff05013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb58952-f208-46ec-8b5f-016ae35ae93d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 1.0363314694166184\n",
      "Epoch 1, Batch 200, Loss: 0.4520766767859459\n",
      "Epoch 1, Batch 300, Loss: 0.3945846852660179\n",
      "Epoch 1, Batch 400, Loss: 0.3408546157926321\n",
      "Epoch 1, Batch 500, Loss: 0.3172695578634739\n",
      "Epoch 1, Batch 600, Loss: 0.2776795271784067\n",
      "Epoch 1, Batch 700, Loss: 0.2810117049515247\n",
      "Epoch 1, Batch 800, Loss: 0.24620795257389547\n",
      "Epoch 1, Batch 900, Loss: 0.24014456316828728\n",
      "Epoch 2, Batch 100, Loss: 0.21239863462746145\n",
      "Epoch 2, Batch 200, Loss: 0.20639328841120005\n",
      "Epoch 2, Batch 300, Loss: 0.2013458077982068\n",
      "Epoch 2, Batch 400, Loss: 0.2005569639801979\n",
      "Epoch 2, Batch 500, Loss: 0.18633607238531114\n",
      "Epoch 2, Batch 600, Loss: 0.17675244860351086\n",
      "Epoch 2, Batch 700, Loss: 0.1663437693193555\n",
      "Epoch 2, Batch 800, Loss: 0.18365859903395176\n",
      "Epoch 2, Batch 900, Loss: 0.16417676351964475\n",
      "Epoch 3, Batch 100, Loss: 0.13156998842954637\n",
      "Epoch 3, Batch 200, Loss: 0.13651179710403086\n",
      "Epoch 3, Batch 300, Loss: 0.14121451638638974\n",
      "Epoch 3, Batch 400, Loss: 0.143949891384691\n",
      "Epoch 3, Batch 500, Loss: 0.14574168421328068\n",
      "Epoch 3, Batch 600, Loss: 0.12820848917588593\n",
      "Epoch 3, Batch 700, Loss: 0.13907697193324567\n",
      "Epoch 3, Batch 800, Loss: 0.12405566051602364\n",
      "Epoch 3, Batch 900, Loss: 0.14297261882573367\n",
      "Epoch 4, Batch 100, Loss: 0.10653198229148984\n",
      "Epoch 4, Batch 200, Loss: 0.10713146821595729\n",
      "Epoch 4, Batch 300, Loss: 0.12038788301870226\n",
      "Epoch 4, Batch 400, Loss: 0.11264612395316362\n",
      "Epoch 4, Batch 500, Loss: 0.11688337857834995\n",
      "Epoch 4, Batch 600, Loss: 0.10111600880511105\n",
      "Epoch 4, Batch 700, Loss: 0.11912811095826328\n",
      "Epoch 4, Batch 800, Loss: 0.1162497871555388\n",
      "Epoch 4, Batch 900, Loss: 0.10178898962214589\n",
      "Epoch 5, Batch 100, Loss: 0.08848585164174437\n",
      "Epoch 5, Batch 200, Loss: 0.11248784916475416\n",
      "Epoch 5, Batch 300, Loss: 0.09462225333787501\n",
      "Epoch 5, Batch 400, Loss: 0.09696206698194146\n",
      "Epoch 5, Batch 500, Loss: 0.09189608246088028\n",
      "Epoch 5, Batch 600, Loss: 0.09029543828219175\n",
      "Epoch 5, Batch 700, Loss: 0.11030673858709633\n",
      "Epoch 5, Batch 800, Loss: 0.0878520807530731\n",
      "Epoch 5, Batch 900, Loss: 0.09435111725702881\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is used for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Default learning rate for Adam is typically 0.001\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d906c-b775-46f4-8ddf-a15c92f97fe2",
   "metadata": {},
   "source": [
    "Step 3. Report on the results in terms of prediction accuracy on the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d37d9ea-f57b-4885-baba-04784ad175bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.9739833333333333%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on train set: { correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e55c906c-cad9-4fc4-ab7f-89ef4b54f7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9643833333333334%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: { correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e681a23-3870-4c13-9724-b935b3163935",
   "metadata": {},
   "source": [
    "### Report on QMNIST Classification with MLP\n",
    "\n",
    "- **Model Type**: Multi-Layer Perceptron (MLP) Neural Network.\n",
    "- **Dataset**: QMNIST (Extended version of the MNIST dataset).\n",
    "- **Goal**: Classify grayscale images of digits (0-9) from the QMNIST dataset.\n",
    "- **Input Layer**: A flattened vector of size `28x28 = 784` representing each pixel of the input image.\n",
    "- **Hidden Layers**: - **First Layer**: 128 neurons, followed by a ReLU activation function. - **Second Layer**: 64 neurons, followed by a ReLU activation function.\n",
    "- **Output Layer**: 10 neurons, corresponding to the 10 digit classes (0-9), with no activation applied (as CrossEntropyLoss includes softmax internally).\n",
    "- **Optimizer**: Adam (Adaptive Moment Estimation) with a learning rate of `0.001`.\n",
    "- **Loss Function**: CrossEntropyLoss.\n",
    "- **Epochs**: 5\n",
    "- **Batch Size**: 64\n",
    "- **Number of mini-batches**: The model prints the loss every 100 mini-batches. The loss values gradually decreased, indicating successful training.\n",
    "- **Accuracy on Train Set**: A training accuracy of `97.40%` suggests that the model is fitting well on the provided data, without major underfitting issues.It has achieved a high prediction accuracy on the training dataset which tells that it was able to learn the patterns in the training data effectively.\n",
    "- **Accuracy on Test Set**: `96.43%` The model is able to correctly classify approximately 96.43% of the test images. This indicates that model is performing well on this task, with a relatively low error rate.\n",
    "- There is a strong performance in both train and test datasets. There is a small difference in accuracy between the them which indicates that the model has learned effectively from the training data and is capable of making accurate predictions on new data, without overfitting.\n",
    "- **Performance**: A test accuracy of **96.43%** is a good as we have simple MLP model with just two hidden layers. \n",
    "- **Generalization**: The model's performance on the test set indicates that it is able to generalizes well and is not overfitting to the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "731d4efd-9261-4d93-9300-d186e3ad3846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and store predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.numpy())  # Store predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c5bff43-1f29-4c46-b472-631ab4fb9580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZ10lEQVR4nO3caXRV5dmH8f8hOZkTxmgM2EQDhihWm5ZBKATEBSigIAEDBQLKUF21rdgqKlUgtsEaKdTWoSVhasoUaWCBgaoMrSQVWKvVAi6rIrMiZZJZEp73g2/uEpJA9jFhiNdvLT54su+9n3M45so+2Wyfc84JAABJDS71AgAAlw+iAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJh6GYVZs2bJ5/PZn+DgYLVo0UIjR47U7t27L8oaEhMTNWLECPvvNWvWyOfzac2aNZ72U1xcrIkTJ+rQoUO1uj5JGjFihBITEy+4XdeuXdWmTZtaOWb5383GjRtrZX9n73Pbtm21sr+hQ4fK5/OpT58+tbK/3/72t/L5fF/rNdyzZ48mTpyof/3rX7Wypgvp2rWrunbtWqPt6vt7wzmnmTNnql27doqMjFRMTIxSU1O1ZMmSWlvn5aReRqHczJkzVVJSojfeeEOjR4/WvHnz1LlzZx07duyiryU1NVUlJSVKTU31NFdcXKxJkybVSRRQ2fLly1VYWKiYmJha22deXp4kafPmzXrnnXcC2seePXs0adKkixYF/M+DDz6oBx98UN27d9fSpUu1aNEiDRkyRMePH7/US6sTwZd6AXWpTZs2+t73vidJ6tatm8rKypSVlaXCwkL94Ac/qHLm+PHjioiIqPW1xMTEqEOHDrW+X9Sew4cPa+zYscrKytL06dNrZZ8bN27Uu+++q969e2v58uXKzc1V+/bta2XfqHuFhYV69dVXtWDBAg0aNMge79mz5yVcVd2q12cK5yr/prx9+3ZJX318EhUVpX//+9/q0aOHoqOj1b17d0nSl19+qWeffVatW7dWaGioYmNjNXLkSO3bt6/CPk+fPq3HHntMcXFxioiI0Pe//32tX7++0rGr+/jonXfeUd++fdW0aVOFhYUpKSlJP/3pTyVJEydO1M9//nNJ0nXXXWcfh529jwULFui2225TZGSkoqKi1LNnT/3zn/+sdPxZs2YpOTlZoaGhSklJ0Zw5cwJ6DauzceNGZWRkKDExUeHh4UpMTNTgwYPttT7XwYMHNXLkSDVp0kSRkZHq27evtm7dWmm7N998U927d1dMTIwiIiLUqVMnvfXWW7W69nKPPvqorrnmGv34xz+utX3m5uZKkqZMmaKOHTtq/vz5Vf6EuXv3bo0ZM0bXXnutQkJCFB8fr/T0dO3du1dr1qxR27ZtJUkjR46098HEiRMlVf9RT1UfD06aNEnt27dXkyZN7GOQ3Nxc1eV9Ma/k98b06dOVmJhYIQj13TcqCh999JEkKTY21h778ssvdffdd+v222/XkiVLNGnSJJ05c0b33HOPpkyZoiFDhmj58uWaMmWK3njjDXXt2lUnTpyw+dGjRysnJ0fDhw/XkiVLNGDAAN177706ePDgBdezcuVKde7cWTt27NDUqVNVVFSkCRMmaO/evZKkUaNG6eGHH5YkLV68WCUlJRU+gvrVr36lwYMH68Ybb9TChQs1d+5cHTlyRJ07d9aWLVvsOLNmzdLIkSOVkpKi1157TRMmTFBWVpZWrVr19V/U/7dt2zYlJydr2rRpWrlypZ577jl9+umnatu2rf773/9W2v6BBx5QgwYN9Oc//1nTpk3T+vXr1bVr1wofk/3pT39Sjx49FBMTo9mzZ2vhwoVq0qSJevbsecH/+csjXP6N80LefPNNzZkzRzNmzFBQUJCXp16tEydOaN68eWrbtq3atGmj+++/X0eOHNGiRYsqbLd79261bdtWf/nLXzRu3DgVFRVp2rRpatiwoQ4ePKjU1FTNnDlTkjRhwgR7H4waNcrzmrZt26axY8dq4cKFWrx4se699149/PDDysrKqpXnXN0xr8T3RmlpqUpKSvSd73xHU6dOVUJCgoKCgnT99dcrJyenTkN6Sbl6aObMmU6S+8c//uFOnz7tjhw54pYtW+ZiY2NddHS0++yzz5xzzmVmZjpJLi8vr8L8vHnznCT32muvVXh8w4YNTpJ76aWXnHPOvf/++06Se+SRRypsl5+f7yS5zMxMe2z16tVOklu9erU9lpSU5JKSktyJEyeqfS7PP/+8k+Q++eSTCo/v2LHDBQcHu4cffrjC40eOHHFxcXFu0KBBzjnnysrKXHx8vEtNTXVnzpyx7bZt2+b8fr9LSEio9tjl0tLS3E033XTB7c5WWlrqjh496iIjI9306dPt8fK/m/79+1fYft26dU6Se/bZZ51zzh07dsw1adLE9e3bt8J2ZWVl7pZbbnHt2rWrtM+zX6M1a9a4oKAgN2nSpAuu9ciRIy4xMdE98cQT9lhCQoLr3bu3p+d8rjlz5jhJ7pVXXrHjREVFuc6dO1fY7v7773d+v99t2bKl2n2Vv/dmzpxZ6WtpaWkuLS2t0uOZmZnn/fstKytzp0+fdpMnT3ZNmzat8P6obp9VHbu+vjc+/fRTJ8nFxMS4Fi1auNmzZ7u33nrL/fCHP3SS3JNPPunpeV8p6vWZQocOHeT3+xUdHa0+ffooLi5ORUVFuvrqqytsN2DAgAr/vWzZMjVq1Eh9+/ZVaWmp/bn11lsVFxdnH9+sXr1akir9fmLQoEEKDj7/r2v+85//6OOPP9YDDzygsLAwz89t5cqVKi0t1fDhwyusMSwsTGlpabbGDz74QHv27NGQIUPk8/lsPiEhQR07dvR83OocPXpUjz/+uFq2bKng4GAFBwcrKipKx44d0/vvv19p+3Nfs44dOyohIcFe0+LiYh04cECZmZkVnt+ZM2fUq1cvbdiw4bwXDKSlpam0tFRPP/30Bdc+fvx4+f3+Gm3rRW5ursLDw5WRkSFJioqK0sCBA/X3v/9dH374oW1XVFSkbt26KSUlpVaPX5VVq1bpjjvuUMOGDRUUFGTPe//+/fr888/r5JhX6nvjzJkzkqQvvvhCixYt0vDhw3X77bfr5ZdfVr9+/TR16lQdPXrU68tx2avXv2ieM2eOUlJSFBwcrKuvvlrXXHNNpW0iIiIqXWmyd+9eHTp0SCEhIVXut/yUd//+/ZKkuLi4Cl8PDg5W06ZNz7u28t9NtGjRomZP5hzlHzGVf9Z8rgYNGpx3jeWP1dZlnEOGDNFbb72lX/ziF2rbtq1iYmLk8/l01113Vfi47exjV/VY+XrLn196enq1xzxw4IAiIyO/1rrXr1+vl156SYsXL9bJkyd18uRJSV99QygtLdWhQ4cUHh6u0NBQT/v96KOP9Le//U0DBgyQc84++khPT9fMmTOVl5en7OxsSV+9FwJ9H3ixfv169ejRQ127dtUf//hHtWjRQiEhISosLNQvf/nLKv+easOV+t5o3LixfD6foqOjK10kcuedd6qwsFBbtmxRu3btvtZxLjf1OgopKSl29VF1zv7puVyzZs3UtGlTrVixosqZ6OhoSbJv/J999pmaN29uXy8tLbU3cHXKf6+xa9eu825XnWbNmkmSCgoKlJCQUO12Z6/xXFU9FojDhw9r2bJleuaZZzR+/Hh7/NSpUzpw4ECVM9Wtp2XLlpL+9/xefPHFaq/aOveMLxBbtmyRc079+/ev9LWdO3eqcePG+s1vfmO//K+pvLw8OedUUFCggoKCSl+fPXu2nn32WQUFBSk2Njbg94EkhYWF6fDhw5UeP/fz+vnz58vv92vZsmUVzk4LCwsDPvaFXMnvjfDwcLVq1arK9bj//31C+Q9f9Um9jkKg+vTpo/nz56usrOy8lw+WX/GRn5+v7373u/b4woULVVpaet5j3HDDDUpKSlJeXp7GjRtX7U+i5Y+f+xNVz549FRwcrI8//rjSx19nS05O1jXXXKN58+Zp3LhxFsHt27eruLhY8fHx511nTfh8PjnnKj2HGTNmqKysrMqZ/Pz8CusuLi7W9u3b7ZennTp1UqNGjbRlyxb96Ec/+tprrE6vXr3sY4mzZWRk6LrrrlN2drZ9M6qpsrIyzZ49W0lJSZoxY0alry9btkwvvPCCioqK1KdPH915552aO3euPvjgAyUnJ1e5z+reB9JX/1By0aJFOnXqlG23f/9+FRcXVzgLLv+HnGf/Iv3EiROaO3eup+fnxZX83pC++mg5OztbxcXFFT5uff311xUVFaWbbrqpTo9/KRCFKmRkZCg/P1933XWXfvKTn6hdu3by+/3atWuXVq9erXvuuUf9+/dXSkqKhg4dqmnTpsnv9+uOO+7Qpk2blJOTU6N//PT73/9effv2VYcOHfTII4/oW9/6lnbs2KGVK1cqPz9fknTzzTdL+urSuMzMTPn9fiUnJysxMVGTJ0/WU089pa1bt6pXr15q3Lix9u7dq/Xr1ysyMlKTJk1SgwYNlJWVpVGjRql///4aPXq0Dh06pIkTJ1Z5ml6dL774osqfeGNjY5WWlqYuXbro+eefV7NmzZSYmKi1a9cqNzdXjRo1qnJ/Gzdu1KhRozRw4EDt3LlTTz31lJo3b66HHnpI0lefv7/44ovKzMzUgQMHlJ6erquuukr79u3Tu+++q3379unll1+udr1r165V9+7d9fTTT5/3s+O4uLgqX4ewsDA1bdq00qWeI0aM0OzZs/XJJ59U+6/Bi4qKtGfPHj333HNVXirapk0b/e53v1Nubq769OmjyZMnq6ioSF26dNGTTz6pm2++WYcOHdKKFSs0btw4tW7dWklJSQoPD1d+fr5SUlIUFRWl+Ph4xcfHa9iwYXr11Vc1dOhQjR49Wvv379evf/3rSu/B3r17a+rUqRoyZIjGjBmj/fv3Kycnx/NHY+eqr+8NSfrZz36m/Px8DRw4UFlZWWrRooUKCgq0dOlS5eTkKDw8vOYv1JXiUv6Wu66UX3GwYcOG826XmZnpIiMjq/za6dOnXU5OjrvllltcWFiYi4qKcq1bt3Zjx451H374oW136tQp9+ijj7qrrrrKhYWFuQ4dOriSkhKXkJBwwauPnHOupKTE3Xnnna5hw4YuNDTUJSUlVbqa6YknnnDx8fGuQYMGlfZRWFjounXr5mJiYlxoaKhLSEhw6enp7s0336ywjxkzZrhWrVq5kJAQd8MNN7i8vLwLXp1SLi0tzUmq8k/5FSq7du1yAwYMcI0bN3bR0dGuV69ebtOmTZVeh/K/m7/+9a9u2LBhrlGjRi48PNzdddddFV7XcmvXrnW9e/d2TZo0cX6/3zVv3tz17t3bLVq0qNI+z77CpPz1fuaZZy74/KpS3dVHAwYMcOHh4e7gwYPVzvbr18+FhIS4zz//vNptMjIyXHBwsF0Jt3PnTnf//fe7uLg45/f7XXx8vBs0aJDbu3evzcybN8+1bt3a+f3+Ss9t9uzZLiUlxYWFhbkbb7zRLViwoMq/37y8PJecnOxCQ0Pd9ddf77Kzs11ubm6l18/L1Uf1/b2xY8cOl5GR4Ro3buxCQkLct7/97UpXLNYnPufq68W2QO2Li4vTsGHD9Pzzz1/qpQB1gigANbR582bddttt2rp1q/2yE6hviAIAwNS/66kAAAEjCgAAQxQAAIYoAABMjf/xWlW3gwAAXDlqcl0RZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJjgS70AADUTFxfneebxxx/3PJOenu55pkWLFp5njh8/7nlGkvr06eN5ZvXq1QEd65uIMwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAIzPOedqtKHPV9drAa44wcHe7ynZr1+/gI41fvx4zzOpqakBHcurQ4cOeZ5ZsmRJQMcK5DUfNmxYQMeqb2ry7Z4zBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAAjPc7SwH1VHR0tOeZV155xfPM4MGDPc8E6uOPP/Y8M336dM8zr7/+uueZrVu3ep5B3eNMAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAww3xUC8FBQV5npkyZYrnmUBubnfs2DHPM5JUXFzseSYrK8vzzNtvv+15BvUHZwoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAw3CUV9dKYMWM8zzz44IN1sJLKCgsLA5obNmxY7S4EqAJnCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGJ9zztVoQ5+vrtcCVCk42Pt9G48ePep5JiQkxPNMQUGB55nBgwd7npGksrKygOaAcjX5ds+ZAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAxvudxoCLbMWKFZ5nArm53c6dOz3P3HfffZ5nangPSuCS4EwBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAADDDfFw0XTq1CmguS5dunieCeSmcy+88MJFOQ5wOeNMAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAMbnanibR5/PV9drwRUkNDTU88x7770X0LFatWrleebtt9/2PBPI3ViBK0lNvt1zpgAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgAm+1AvAlWno0KGeZwK5sZ0klZWVeZ65++67AzoW8E3HmQIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYb4kERERGeZx577LE6WEnVNm/e7HmmWbNmnmdatmzpeebkyZOeZzZt2uR5BrhYOFMAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMBwQzzooYce8jzTqlWrOlhJ1SIjIz3PvPfee55nwsLCPM+UlpZ6nlm1apXnGUlat26d55ns7GzPM4E8J9QfnCkAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGC4IR40fPjwS72E80pKSrooxykpKfE8c+2113qeuf766z3PSFKPHj08z5w4ccLzTE5OjucZ1B+cKQAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMBwl1Rc9kpLSz3PZGdne56ZOHGi5xmfz+d5JlDTp0/3PPPcc895nikoKPA8s23bNs8zuDxxpgAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgOGGeLhoNm/eHNDciy++6HnmD3/4Q0DH8so5d1GOI0l+v9/zTCA37OvUqZPnGW6IV39wpgAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgOGGeLhoevfuHdDcjh07ankll9bVV18d0Nx9991XyyupWlRU1EU5Di5PnCkAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGC4IR50+PDhi3KcgQMHBjT3wgsv1PJKLq309PSA5ho2bFjLK6naunXrLspxcHniTAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAONzzrkabejz1fVacInceuutnmdKSko8z5SWlnqekaT27dt7ntmyZYvnmZCQEM8zY8aM8TwT6A3+/H6/55mCggLPM4MGDfI8gytDTb7dc6YAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAw11SEZBu3bp5nlm6dGlAxwoKCvI8c/jwYc8zDRp4/xkpNjbW80ygDh486HkmLS3N88ymTZs8z+DKwF1SAQCeEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhhvi4aKJiIgIaK59+/aeZ/Lz8z3PxMXFeZ4JxPTp0wOamzx5sueZQG6ih/qLG+IBADwhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMN8QDgG8IbogHAPCEKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwwTXd0DlXl+sAAFwGOFMAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAAJj/AzL+GuH5UGK1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get one test image and its label\n",
    "image, label = images[1], labels[1]\n",
    "\n",
    "# Reshape the image tensor to a 28x28 shape\n",
    "image = image.view(28, 28)\n",
    "\n",
    "# Convert the image tensor to a numpy array for visualization\n",
    "image_numpy = image.numpy()\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(image_numpy, cmap='gray')\n",
    "plt.title(f'Predicted Label: {predictions[1]}, Actual Label: {label.item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbada375-982c-4667-80b4-c5cd532eb789",
   "metadata": {},
   "source": [
    "\n",
    "Further experimentation with model architecture, optimizers, and regularization techniques could potentially improve performance beyond the current result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df396259-57f3-4d5f-8e20-c3df860d71c5",
   "metadata": {},
   "source": [
    "### Step 4.One of the proposed modifications below: Increase the current number of nodes in the layer to 256  nodes\n",
    "Hypothesize how it would change the performance results?\n",
    "\n",
    "### Hypothesis\n",
    "### In this case, Increasing the current number of nodes in the layer to 256: \n",
    "- With increase in nodes from 128 to 256 nodes the network should be able to capture more patterns in the data than with 128 nodes, potentially improving performance on complex tasks or larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e7597-06e4-4f73-8f0f-3b69292cd52b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Step 5. Modify the model based on the chosen method and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a15381ad-4821-490c-bc08-67c23f8645ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)  # Changed from 128 to 256\n",
    "        self.fc2 = nn.Linear(256, 64)       # Updated input size to 256 to match fc1's output size\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea5daa25-5f5c-42b9-9cf9-0e4ee5aa126d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.06905673784669489\n",
      "Epoch 1, Batch 200, Loss: 0.07731718733441084\n",
      "Epoch 1, Batch 300, Loss: 0.07227643823483959\n",
      "Epoch 1, Batch 400, Loss: 0.06974336553364992\n",
      "Epoch 1, Batch 500, Loss: 0.06796841453993693\n",
      "Epoch 1, Batch 600, Loss: 0.07688596389256418\n",
      "Epoch 1, Batch 700, Loss: 0.07492174223996699\n",
      "Epoch 1, Batch 800, Loss: 0.07354238281026483\n",
      "Epoch 1, Batch 900, Loss: 0.08953806438483297\n",
      "Epoch 2, Batch 100, Loss: 0.047462991760112344\n",
      "Epoch 2, Batch 200, Loss: 0.07254781841067598\n",
      "Epoch 2, Batch 300, Loss: 0.0630319649213925\n",
      "Epoch 2, Batch 400, Loss: 0.05263380414107814\n",
      "Epoch 2, Batch 500, Loss: 0.06388017681892962\n",
      "Epoch 2, Batch 600, Loss: 0.07927875078748911\n",
      "Epoch 2, Batch 700, Loss: 0.05163418082054704\n",
      "Epoch 2, Batch 800, Loss: 0.06942413487005979\n",
      "Epoch 2, Batch 900, Loss: 0.06943638281896711\n",
      "Epoch 3, Batch 100, Loss: 0.043689871069509535\n",
      "Epoch 3, Batch 200, Loss: 0.05400549648911692\n",
      "Epoch 3, Batch 300, Loss: 0.05164327190257609\n",
      "Epoch 3, Batch 400, Loss: 0.05664482595864683\n",
      "Epoch 3, Batch 500, Loss: 0.06482999538071453\n",
      "Epoch 3, Batch 600, Loss: 0.05063487593084574\n",
      "Epoch 3, Batch 700, Loss: 0.05866231518331915\n",
      "Epoch 3, Batch 800, Loss: 0.05483445315388963\n",
      "Epoch 3, Batch 900, Loss: 0.06709402221022173\n",
      "Epoch 4, Batch 100, Loss: 0.04221135801344644\n",
      "Epoch 4, Batch 200, Loss: 0.05410702804801985\n",
      "Epoch 4, Batch 300, Loss: 0.04840271935565397\n",
      "Epoch 4, Batch 400, Loss: 0.052534630770096555\n",
      "Epoch 4, Batch 500, Loss: 0.051692980646621436\n",
      "Epoch 4, Batch 600, Loss: 0.0495856632781215\n",
      "Epoch 4, Batch 700, Loss: 0.041976850983919574\n",
      "Epoch 4, Batch 800, Loss: 0.05824062747065909\n",
      "Epoch 4, Batch 900, Loss: 0.05170624592574313\n",
      "Epoch 5, Batch 100, Loss: 0.044460943260928615\n",
      "Epoch 5, Batch 200, Loss: 0.04395428406511201\n",
      "Epoch 5, Batch 300, Loss: 0.0338294577592751\n",
      "Epoch 5, Batch 400, Loss: 0.04772431459510699\n",
      "Epoch 5, Batch 500, Loss: 0.05410153440432623\n",
      "Epoch 5, Batch 600, Loss: 0.048406741333019455\n",
      "Epoch 5, Batch 700, Loss: 0.045080808855127545\n",
      "Epoch 5, Batch 800, Loss: 0.04566441950970329\n",
      "Epoch 5, Batch 900, Loss: 0.04159237893181853\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8541a210-7e82-4eb0-b029-9acbad662f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9723666666666667%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: { correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1cdc44-0fcb-4438-a802-a3bbce8f8470",
   "metadata": {},
   "source": [
    "Step 6. Report on the results of the modified model and if it matches your hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc5a28-089e-40cc-b8db-20d4a598b6bd",
   "metadata": {},
   "source": [
    "#### **Results of the Modified Model:**\n",
    "- **Previous Test Accuracy (before modification)**: 96.43% \n",
    "- **New Test Accuracy (after increasing the number of neurons in the hidden layer from 128 to 256)**: **97.24%**\n",
    "\n",
    "##### **Hypothesis**:\n",
    "We predicted that increasing the number of neurons in the hidden layer from 128 to 256 would increase the capacity of the network to learn more complex patterns in the data and will potentially improve the performance.\n",
    "##### **Outcome**:\n",
    "The test accuracy improved by **0.80%** (from 96.43% to 97.24%), which is consistent with the hypothesis and it indicates that increasing the number of nodes helped model to utilize the increase capacity to learn better representations of the data which lead to better generalization on the test set.\n",
    "\n",
    "### **Conclusion**:\n",
    "The result matches the hypothesis without overfitting, leading to an increase in test accuracy and helped the model achieve better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37451e6c-b3ce-43c3-a288-9098488d2b5b",
   "metadata": {},
   "source": [
    "Step 7. Experiment with different optimizers, loss functions, dropout, and activation functions, and observe the change in performance as you tune these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9efaec3c-a557-4d2b-adf0-46c3ce345c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the MLP model with dynamic activation function and dropout\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, activation_fn=nn.ReLU, dropout_rate=0.0, loss_function=nn.CrossEntropyLoss):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        self.activation_fn = activation_fn()\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.activation_fn(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc4(x)\n",
    "        if isinstance(self.loss_function, nn.NLLLoss):\n",
    "            x = torch.log_softmax(x, dim=1)  # Apply log-softmax for NLLLoss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "41f61efe-94eb-4abf-8041-a5cf479b4d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to choose optimizer\n",
    "def get_optimizer(optimizer_name, model):\n",
    "    if optimizer_name == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=0.001)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        return optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        return optim.RMSprop(model.parameters(), lr=0.001)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9daf0dfb-169e-437c-bf34-1459987a85a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # print every 100 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc2aa8e6-1622-499f-9f39-b1865feec389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e98b6361-9746-415a-8ecf-91bc016b80a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to run experiments on both hyperparameter lists\n",
    "def run_experiment(hyperparameters):\n",
    "    for optimizer_name in hyperparameters['optimizers']:\n",
    "        for loss_fn in hyperparameters['loss_functions']:\n",
    "            for dropout_rate in hyperparameters['dropout_rates']:\n",
    "                for activation_fn in hyperparameters['activation_functions']:\n",
    "                    print(f\"Experiment with: Optimizer={optimizer_name}, Loss={loss_fn.__class__.__name__}, Dropout={dropout_rate}, Activation={activation_fn.__name__}\")\n",
    "                    \n",
    "                    # Initialize the model\n",
    "                    model = MLP(activation_fn=activation_fn, dropout_rate=dropout_rate, loss_function=loss_fn)\n",
    "                    \n",
    "                    # Get the optimizer\n",
    "                    optimizer = get_optimizer(optimizer_name, model)\n",
    "                    \n",
    "                    # Train the model\n",
    "                    train_model(model, train_loader, loss_fn, optimizer, epochs=5)\n",
    "                    \n",
    "                    # Evaluate the model\n",
    "                    accuracy = evaluate_model(model, test_loader)\n",
    "                    print(f\"Result: Accuracy = {accuracy:.2f}%\")\n",
    "                    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac2af89b-712c-4872-be7f-0462a03a4966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments for Hyperparameter List 1:\n",
      "Experiment with: Optimizer=rmsprop, Loss=CrossEntropyLoss, Dropout=0.2, Activation=LeakyReLU\n",
      "Epoch 1, Batch 100, Loss: 1.1337432888150216\n",
      "Epoch 1, Batch 200, Loss: 0.5158341240882873\n",
      "Epoch 1, Batch 300, Loss: 0.4251799687743187\n",
      "Epoch 1, Batch 400, Loss: 0.3848441827297211\n",
      "Epoch 1, Batch 500, Loss: 0.3436297746002674\n",
      "Epoch 1, Batch 600, Loss: 0.3199358186870813\n",
      "Epoch 1, Batch 700, Loss: 0.2993918929994106\n",
      "Epoch 1, Batch 800, Loss: 0.270193412527442\n",
      "Epoch 1, Batch 900, Loss: 0.2510120403766632\n",
      "Epoch 2, Batch 100, Loss: 0.24260803170502185\n",
      "Epoch 2, Batch 200, Loss: 0.22601986669003962\n",
      "Epoch 2, Batch 300, Loss: 0.23331793956458569\n",
      "Epoch 2, Batch 400, Loss: 0.21184616539627313\n",
      "Epoch 2, Batch 500, Loss: 0.22076430149376391\n",
      "Epoch 2, Batch 600, Loss: 0.2049866520613432\n",
      "Epoch 2, Batch 700, Loss: 0.21310178626328707\n",
      "Epoch 2, Batch 800, Loss: 0.19103831101208926\n",
      "Epoch 2, Batch 900, Loss: 0.21507610529661178\n",
      "Epoch 3, Batch 100, Loss: 0.19108140882104635\n",
      "Epoch 3, Batch 200, Loss: 0.1807988523505628\n",
      "Epoch 3, Batch 300, Loss: 0.1727473097667098\n",
      "Epoch 3, Batch 400, Loss: 0.17502893607132136\n",
      "Epoch 3, Batch 500, Loss: 0.16223132770508528\n",
      "Epoch 3, Batch 600, Loss: 0.17472162883728742\n",
      "Epoch 3, Batch 700, Loss: 0.15715261910110712\n",
      "Epoch 3, Batch 800, Loss: 0.1593444265425205\n",
      "Epoch 3, Batch 900, Loss: 0.16360007951036096\n",
      "Epoch 4, Batch 100, Loss: 0.16654103938490153\n",
      "Epoch 4, Batch 200, Loss: 0.16076823411509394\n",
      "Epoch 4, Batch 300, Loss: 0.14760170424357055\n",
      "Epoch 4, Batch 400, Loss: 0.1479653554223478\n",
      "Epoch 4, Batch 500, Loss: 0.1654143869318068\n",
      "Epoch 4, Batch 600, Loss: 0.14231195103377103\n",
      "Epoch 4, Batch 700, Loss: 0.1406181070022285\n",
      "Epoch 4, Batch 800, Loss: 0.13936788469552994\n",
      "Epoch 4, Batch 900, Loss: 0.15534283243119718\n",
      "Epoch 5, Batch 100, Loss: 0.13545708590187133\n",
      "Epoch 5, Batch 200, Loss: 0.14139451121911406\n",
      "Epoch 5, Batch 300, Loss: 0.12604623631574213\n",
      "Epoch 5, Batch 400, Loss: 0.14803586763329804\n",
      "Epoch 5, Batch 500, Loss: 0.12775276493281126\n",
      "Epoch 5, Batch 600, Loss: 0.1338175136782229\n",
      "Epoch 5, Batch 700, Loss: 0.13823900589719415\n",
      "Epoch 5, Batch 800, Loss: 0.13458568101748825\n",
      "Epoch 5, Batch 900, Loss: 0.12424086853861808\n",
      "Finished Training\n",
      "Accuracy on test set: 96.73%\n",
      "Result: Accuracy = 96.73%\n",
      "--------------------------------------------------\n",
      "Experiment with: Optimizer=rmsprop, Loss=CrossEntropyLoss, Dropout=0.2, Activation=Sigmoid\n",
      "Epoch 1, Batch 100, Loss: 1.4536260920763016\n",
      "Epoch 1, Batch 200, Loss: 0.7279765844345093\n",
      "Epoch 1, Batch 300, Loss: 0.534746660888195\n",
      "Epoch 1, Batch 400, Loss: 0.44543316423892976\n",
      "Epoch 1, Batch 500, Loss: 0.40754905954003334\n",
      "Epoch 1, Batch 600, Loss: 0.36050122290849684\n",
      "Epoch 1, Batch 700, Loss: 0.3542899763584137\n",
      "Epoch 1, Batch 800, Loss: 0.3244700839370489\n",
      "Epoch 1, Batch 900, Loss: 0.31279375597834586\n",
      "Epoch 2, Batch 100, Loss: 0.2799248827248812\n",
      "Epoch 2, Batch 200, Loss: 0.2498957382887602\n",
      "Epoch 2, Batch 300, Loss: 0.26321930676698685\n",
      "Epoch 2, Batch 400, Loss: 0.2478557004779577\n",
      "Epoch 2, Batch 500, Loss: 0.25751659348607064\n",
      "Epoch 2, Batch 600, Loss: 0.23687447004020215\n",
      "Epoch 2, Batch 700, Loss: 0.22490404553711416\n",
      "Epoch 2, Batch 800, Loss: 0.23911519262939693\n",
      "Epoch 2, Batch 900, Loss: 0.22057087440043688\n",
      "Epoch 3, Batch 100, Loss: 0.20660481937229633\n",
      "Epoch 3, Batch 200, Loss: 0.18980611629784108\n",
      "Epoch 3, Batch 300, Loss: 0.1976116244122386\n",
      "Epoch 3, Batch 400, Loss: 0.17703411456197501\n",
      "Epoch 3, Batch 500, Loss: 0.1952472685277462\n",
      "Epoch 3, Batch 600, Loss: 0.17078501377254723\n",
      "Epoch 3, Batch 700, Loss: 0.1776699212566018\n",
      "Epoch 3, Batch 800, Loss: 0.18097829967737197\n",
      "Epoch 3, Batch 900, Loss: 0.16358590986579657\n",
      "Epoch 4, Batch 100, Loss: 0.1589263989776373\n",
      "Epoch 4, Batch 200, Loss: 0.15216256881132723\n",
      "Epoch 4, Batch 300, Loss: 0.15664451241493224\n",
      "Epoch 4, Batch 400, Loss: 0.15902839740738273\n",
      "Epoch 4, Batch 500, Loss: 0.14206006679683925\n",
      "Epoch 4, Batch 600, Loss: 0.15078540936112403\n",
      "Epoch 4, Batch 700, Loss: 0.15649657910689713\n",
      "Epoch 4, Batch 800, Loss: 0.14250470912083985\n",
      "Epoch 4, Batch 900, Loss: 0.14777418613433838\n",
      "Epoch 5, Batch 100, Loss: 0.1387975824251771\n",
      "Epoch 5, Batch 200, Loss: 0.12646669602021576\n",
      "Epoch 5, Batch 300, Loss: 0.1357408046722412\n",
      "Epoch 5, Batch 400, Loss: 0.12532551592215896\n",
      "Epoch 5, Batch 500, Loss: 0.14297344498336315\n",
      "Epoch 5, Batch 600, Loss: 0.13537923572584987\n",
      "Epoch 5, Batch 700, Loss: 0.12762548943981528\n",
      "Epoch 5, Batch 800, Loss: 0.12110664611682295\n",
      "Epoch 5, Batch 900, Loss: 0.12700539665296673\n",
      "Finished Training\n",
      "Accuracy on test set: 96.34%\n",
      "Result: Accuracy = 96.34%\n",
      "--------------------------------------------------\n",
      "\n",
      "Running experiments for Hyperparameter List 2:\n",
      "Experiment with: Optimizer=sgd, Loss=NLLLoss, Dropout=0.3, Activation=ReLU\n",
      "Epoch 1, Batch 100, Loss: 1.5040160751342773\n",
      "Epoch 1, Batch 200, Loss: 0.6630196419358253\n",
      "Epoch 1, Batch 300, Loss: 0.5354073125123978\n",
      "Epoch 1, Batch 400, Loss: 0.4383344121277332\n",
      "Epoch 1, Batch 500, Loss: 0.40448505744338037\n",
      "Epoch 1, Batch 600, Loss: 0.42512585073709486\n",
      "Epoch 1, Batch 700, Loss: 0.36416645810008047\n",
      "Epoch 1, Batch 800, Loss: 0.3354758441448212\n",
      "Epoch 1, Batch 900, Loss: 0.3211989791691303\n",
      "Epoch 2, Batch 100, Loss: 0.309370616748929\n",
      "Epoch 2, Batch 200, Loss: 0.28851501412689684\n",
      "Epoch 2, Batch 300, Loss: 0.27618732750415803\n",
      "Epoch 2, Batch 400, Loss: 0.2614491730928421\n",
      "Epoch 2, Batch 500, Loss: 0.24044117331504822\n",
      "Epoch 2, Batch 600, Loss: 0.2633632542192936\n",
      "Epoch 2, Batch 700, Loss: 0.2622232184559107\n",
      "Epoch 2, Batch 800, Loss: 0.23845640815794467\n",
      "Epoch 2, Batch 900, Loss: 0.24440097138285638\n",
      "Epoch 3, Batch 100, Loss: 0.23366123020648957\n",
      "Epoch 3, Batch 200, Loss: 0.2546696024388075\n",
      "Epoch 3, Batch 300, Loss: 0.22377079673111439\n",
      "Epoch 3, Batch 400, Loss: 0.20591039959341287\n",
      "Epoch 3, Batch 500, Loss: 0.20928369745612144\n",
      "Epoch 3, Batch 600, Loss: 0.18322633180767298\n",
      "Epoch 3, Batch 700, Loss: 0.20122212857007982\n",
      "Epoch 3, Batch 800, Loss: 0.18433678029105066\n",
      "Epoch 3, Batch 900, Loss: 0.20476795423775912\n",
      "Epoch 4, Batch 100, Loss: 0.19642710991203785\n",
      "Epoch 4, Batch 200, Loss: 0.1815755658596754\n",
      "Epoch 4, Batch 300, Loss: 0.1843268320336938\n",
      "Epoch 4, Batch 400, Loss: 0.1835954551771283\n",
      "Epoch 4, Batch 500, Loss: 0.19819079669192433\n",
      "Epoch 4, Batch 600, Loss: 0.17930118806660175\n",
      "Epoch 4, Batch 700, Loss: 0.1574723694473505\n",
      "Epoch 4, Batch 800, Loss: 0.19650463584810496\n",
      "Epoch 4, Batch 900, Loss: 0.17469872538000344\n",
      "Epoch 5, Batch 100, Loss: 0.16253195268101991\n",
      "Epoch 5, Batch 200, Loss: 0.1687688677199185\n",
      "Epoch 5, Batch 300, Loss: 0.1714892105385661\n",
      "Epoch 5, Batch 400, Loss: 0.15622288094833492\n",
      "Epoch 5, Batch 500, Loss: 0.16878377348184587\n",
      "Epoch 5, Batch 600, Loss: 0.16726962430402637\n",
      "Epoch 5, Batch 700, Loss: 0.1594838596507907\n",
      "Epoch 5, Batch 800, Loss: 0.1667191094905138\n",
      "Epoch 5, Batch 900, Loss: 0.15166211884468794\n",
      "Finished Training\n",
      "Accuracy on test set: 96.62%\n",
      "Result: Accuracy = 96.62%\n",
      "--------------------------------------------------\n",
      "Experiment with: Optimizer=sgd, Loss=NLLLoss, Dropout=0.3, Activation=Tanh\n",
      "Epoch 1, Batch 100, Loss: 1.241654607653618\n",
      "Epoch 1, Batch 200, Loss: 0.5307985877990723\n",
      "Epoch 1, Batch 300, Loss: 0.46247931659221647\n",
      "Epoch 1, Batch 400, Loss: 0.3890121605992317\n",
      "Epoch 1, Batch 500, Loss: 0.3780565986037254\n",
      "Epoch 1, Batch 600, Loss: 0.377501517534256\n",
      "Epoch 1, Batch 700, Loss: 0.36786516904830935\n",
      "Epoch 1, Batch 800, Loss: 0.3554458826780319\n",
      "Epoch 1, Batch 900, Loss: 0.3257698409259319\n",
      "Epoch 2, Batch 100, Loss: 0.31001684404909613\n",
      "Epoch 2, Batch 200, Loss: 0.332362797036767\n",
      "Epoch 2, Batch 300, Loss: 0.2822479337453842\n",
      "Epoch 2, Batch 400, Loss: 0.2772634647786617\n",
      "Epoch 2, Batch 500, Loss: 0.3002275551110506\n",
      "Epoch 2, Batch 600, Loss: 0.25541371695697307\n",
      "Epoch 2, Batch 700, Loss: 0.27030453346669675\n",
      "Epoch 2, Batch 800, Loss: 0.26431567169725895\n",
      "Epoch 2, Batch 900, Loss: 0.2554474803805351\n",
      "Epoch 3, Batch 100, Loss: 0.24511410258710384\n",
      "Epoch 3, Batch 200, Loss: 0.23131850756704808\n",
      "Epoch 3, Batch 300, Loss: 0.24079983454197645\n",
      "Epoch 3, Batch 400, Loss: 0.26134974136948586\n",
      "Epoch 3, Batch 500, Loss: 0.22871704638004303\n",
      "Epoch 3, Batch 600, Loss: 0.23364653900265694\n",
      "Epoch 3, Batch 700, Loss: 0.20452327843755483\n",
      "Epoch 3, Batch 800, Loss: 0.2358721635490656\n",
      "Epoch 3, Batch 900, Loss: 0.2076486661285162\n",
      "Epoch 4, Batch 100, Loss: 0.2136654247716069\n",
      "Epoch 4, Batch 200, Loss: 0.21027543261647225\n",
      "Epoch 4, Batch 300, Loss: 0.19257252935320138\n",
      "Epoch 4, Batch 400, Loss: 0.2008698683604598\n",
      "Epoch 4, Batch 500, Loss: 0.20037575475871564\n",
      "Epoch 4, Batch 600, Loss: 0.2283575750142336\n",
      "Epoch 4, Batch 700, Loss: 0.21729778744280337\n",
      "Epoch 4, Batch 800, Loss: 0.20065213467925788\n",
      "Epoch 4, Batch 900, Loss: 0.18680509142577648\n",
      "Epoch 5, Batch 100, Loss: 0.1905361970514059\n",
      "Epoch 5, Batch 200, Loss: 0.16472241558134557\n",
      "Epoch 5, Batch 300, Loss: 0.1718336848542094\n",
      "Epoch 5, Batch 400, Loss: 0.19008958335965873\n",
      "Epoch 5, Batch 500, Loss: 0.18133452013134957\n",
      "Epoch 5, Batch 600, Loss: 0.18363360684365035\n",
      "Epoch 5, Batch 700, Loss: 0.1696646638587117\n",
      "Epoch 5, Batch 800, Loss: 0.166577426828444\n",
      "Epoch 5, Batch 900, Loss: 0.17072433467954398\n",
      "Finished Training\n",
      "Accuracy on test set: 95.79%\n",
      "Result: Accuracy = 95.79%\n",
      "--------------------------------------------------\n",
      "\n",
      "Running experiments for Hyperparameter List 3:\n",
      "Experiment with: Optimizer=adam, Loss=CrossEntropyLoss, Dropout=0.4, Activation=ReLU\n",
      "Epoch 1, Batch 100, Loss: 1.3001762866973876\n",
      "Epoch 1, Batch 200, Loss: 0.6952383545041084\n",
      "Epoch 1, Batch 300, Loss: 0.5439148098230362\n",
      "Epoch 1, Batch 400, Loss: 0.4863620008528233\n",
      "Epoch 1, Batch 500, Loss: 0.4539833667874336\n",
      "Epoch 1, Batch 600, Loss: 0.40874165639281274\n",
      "Epoch 1, Batch 700, Loss: 0.4115827763080597\n",
      "Epoch 1, Batch 800, Loss: 0.3832277475297451\n",
      "Epoch 1, Batch 900, Loss: 0.3763863322138786\n",
      "Epoch 2, Batch 100, Loss: 0.3402458621561527\n",
      "Epoch 2, Batch 200, Loss: 0.3499644219875336\n",
      "Epoch 2, Batch 300, Loss: 0.31739722296595574\n",
      "Epoch 2, Batch 400, Loss: 0.33257250487804413\n",
      "Epoch 2, Batch 500, Loss: 0.30783938616514206\n",
      "Epoch 2, Batch 600, Loss: 0.31175900585949423\n",
      "Epoch 2, Batch 700, Loss: 0.2761188127100468\n",
      "Epoch 2, Batch 800, Loss: 0.3310072224587202\n",
      "Epoch 2, Batch 900, Loss: 0.29317094437777996\n",
      "Epoch 3, Batch 100, Loss: 0.27392201833426955\n",
      "Epoch 3, Batch 200, Loss: 0.26170946039259435\n",
      "Epoch 3, Batch 300, Loss: 0.2951350428164005\n",
      "Epoch 3, Batch 400, Loss: 0.25855422161519526\n",
      "Epoch 3, Batch 500, Loss: 0.27626391433179376\n",
      "Epoch 3, Batch 600, Loss: 0.2826261060684919\n",
      "Epoch 3, Batch 700, Loss: 0.2669871794432402\n",
      "Epoch 3, Batch 800, Loss: 0.2759283358603716\n",
      "Epoch 3, Batch 900, Loss: 0.26251623686403036\n",
      "Epoch 4, Batch 100, Loss: 0.24589213620871306\n",
      "Epoch 4, Batch 200, Loss: 0.24865432776510715\n",
      "Epoch 4, Batch 300, Loss: 0.24056504849344493\n",
      "Epoch 4, Batch 400, Loss: 0.24787115819752217\n",
      "Epoch 4, Batch 500, Loss: 0.23673501431941987\n",
      "Epoch 4, Batch 600, Loss: 0.23294791810214519\n",
      "Epoch 4, Batch 700, Loss: 0.23125937331467866\n",
      "Epoch 4, Batch 800, Loss: 0.2400926462560892\n",
      "Epoch 4, Batch 900, Loss: 0.23006164349615574\n",
      "Epoch 5, Batch 100, Loss: 0.2367312977463007\n",
      "Epoch 5, Batch 200, Loss: 0.22006300266832113\n",
      "Epoch 5, Batch 300, Loss: 0.24024555429816247\n",
      "Epoch 5, Batch 400, Loss: 0.23328698959201574\n",
      "Epoch 5, Batch 500, Loss: 0.23079469226300717\n",
      "Epoch 5, Batch 600, Loss: 0.22886412553489208\n",
      "Epoch 5, Batch 700, Loss: 0.21529911551624537\n",
      "Epoch 5, Batch 800, Loss: 0.22413466531783344\n",
      "Epoch 5, Batch 900, Loss: 0.21207237806171178\n",
      "Finished Training\n",
      "Accuracy on test set: 96.21%\n",
      "Result: Accuracy = 96.21%\n",
      "--------------------------------------------------\n",
      "Experiment with: Optimizer=adam, Loss=CrossEntropyLoss, Dropout=0.4, Activation=Sigmoid\n",
      "Epoch 1, Batch 100, Loss: 1.9719542419910432\n",
      "Epoch 1, Batch 200, Loss: 1.175812383890152\n",
      "Epoch 1, Batch 300, Loss: 0.8158475518226623\n",
      "Epoch 1, Batch 400, Loss: 0.6632151973247528\n",
      "Epoch 1, Batch 500, Loss: 0.5606156167387962\n",
      "Epoch 1, Batch 600, Loss: 0.5176148262619972\n",
      "Epoch 1, Batch 700, Loss: 0.4980233472585678\n",
      "Epoch 1, Batch 800, Loss: 0.4295497870445251\n",
      "Epoch 1, Batch 900, Loss: 0.4169810806214809\n",
      "Epoch 2, Batch 100, Loss: 0.4106122563779354\n",
      "Epoch 2, Batch 200, Loss: 0.37975253373384477\n",
      "Epoch 2, Batch 300, Loss: 0.35422307342290876\n",
      "Epoch 2, Batch 400, Loss: 0.324135804772377\n",
      "Epoch 2, Batch 500, Loss: 0.3445635786652565\n",
      "Epoch 2, Batch 600, Loss: 0.33604484468698503\n",
      "Epoch 2, Batch 700, Loss: 0.32515735521912575\n",
      "Epoch 2, Batch 800, Loss: 0.3172519040107727\n",
      "Epoch 2, Batch 900, Loss: 0.31137781918048857\n",
      "Epoch 3, Batch 100, Loss: 0.28050452291965483\n",
      "Epoch 3, Batch 200, Loss: 0.27944942340254786\n",
      "Epoch 3, Batch 300, Loss: 0.2857575652003288\n",
      "Epoch 3, Batch 400, Loss: 0.28635261550545693\n",
      "Epoch 3, Batch 500, Loss: 0.28203554086387156\n",
      "Epoch 3, Batch 600, Loss: 0.26417093999683855\n",
      "Epoch 3, Batch 700, Loss: 0.2510730366408825\n",
      "Epoch 3, Batch 800, Loss: 0.24885824255645275\n",
      "Epoch 3, Batch 900, Loss: 0.24471110347658395\n",
      "Epoch 4, Batch 100, Loss: 0.2361738819628954\n",
      "Epoch 4, Batch 200, Loss: 0.2438537173718214\n",
      "Epoch 4, Batch 300, Loss: 0.23785017397254704\n",
      "Epoch 4, Batch 400, Loss: 0.2393610592931509\n",
      "Epoch 4, Batch 500, Loss: 0.23412592142820357\n",
      "Epoch 4, Batch 600, Loss: 0.235524480342865\n",
      "Epoch 4, Batch 700, Loss: 0.22183539308607578\n",
      "Epoch 4, Batch 800, Loss: 0.22061925135552884\n",
      "Epoch 4, Batch 900, Loss: 0.22510924324393272\n",
      "Epoch 5, Batch 100, Loss: 0.21069535434246064\n",
      "Epoch 5, Batch 200, Loss: 0.2167888044565916\n",
      "Epoch 5, Batch 300, Loss: 0.21746750868856907\n",
      "Epoch 5, Batch 400, Loss: 0.19835594039410354\n",
      "Epoch 5, Batch 500, Loss: 0.20500899620354177\n",
      "Epoch 5, Batch 600, Loss: 0.2066918421536684\n",
      "Epoch 5, Batch 700, Loss: 0.19268946342170237\n",
      "Epoch 5, Batch 800, Loss: 0.2050872650742531\n",
      "Epoch 5, Batch 900, Loss: 0.2122185654938221\n",
      "Finished Training\n",
      "Accuracy on test set: 95.93%\n",
      "Result: Accuracy = 95.93%\n",
      "--------------------------------------------------\n",
      "\n",
      "Running experiments for Hyperparameter List 4:\n",
      "Experiment with: Optimizer=sgd, Loss=CrossEntropyLoss, Dropout=0.0, Activation=ReLU\n",
      "Epoch 1, Batch 100, Loss: 1.4095047143101693\n",
      "Epoch 1, Batch 200, Loss: 0.46047866538167\n",
      "Epoch 1, Batch 300, Loss: 0.3736083322763443\n",
      "Epoch 1, Batch 400, Loss: 0.34928624495863914\n",
      "Epoch 1, Batch 500, Loss: 0.31287146285176276\n",
      "Epoch 1, Batch 600, Loss: 0.2928739508986473\n",
      "Epoch 1, Batch 700, Loss: 0.239993689507246\n",
      "Epoch 1, Batch 800, Loss: 0.24019295103847982\n",
      "Epoch 1, Batch 900, Loss: 0.23308385893702507\n",
      "Epoch 2, Batch 100, Loss: 0.1960623598098755\n",
      "Epoch 2, Batch 200, Loss: 0.17793603129684926\n",
      "Epoch 2, Batch 300, Loss: 0.19194161288440229\n",
      "Epoch 2, Batch 400, Loss: 0.17897987600415946\n",
      "Epoch 2, Batch 500, Loss: 0.16311546858400106\n",
      "Epoch 2, Batch 600, Loss: 0.1572383975237608\n",
      "Epoch 2, Batch 700, Loss: 0.14035862755030393\n",
      "Epoch 2, Batch 800, Loss: 0.14597828403115273\n",
      "Epoch 2, Batch 900, Loss: 0.14104637388139962\n",
      "Epoch 3, Batch 100, Loss: 0.12583191899582744\n",
      "Epoch 3, Batch 200, Loss: 0.12569278974086046\n",
      "Epoch 3, Batch 300, Loss: 0.12819427544251083\n",
      "Epoch 3, Batch 400, Loss: 0.12950068863108755\n",
      "Epoch 3, Batch 500, Loss: 0.10572182587347925\n",
      "Epoch 3, Batch 600, Loss: 0.11022707473486662\n",
      "Epoch 3, Batch 700, Loss: 0.12949430780485272\n",
      "Epoch 3, Batch 800, Loss: 0.1209978199750185\n",
      "Epoch 3, Batch 900, Loss: 0.11444748256355525\n",
      "Epoch 4, Batch 100, Loss: 0.08825678411871195\n",
      "Epoch 4, Batch 200, Loss: 0.0878704520035535\n",
      "Epoch 4, Batch 300, Loss: 0.08554998607374728\n",
      "Epoch 4, Batch 400, Loss: 0.08953213645145297\n",
      "Epoch 4, Batch 500, Loss: 0.10600413218140602\n",
      "Epoch 4, Batch 600, Loss: 0.0986730767507106\n",
      "Epoch 4, Batch 700, Loss: 0.09286650352180004\n",
      "Epoch 4, Batch 800, Loss: 0.0944680961407721\n",
      "Epoch 4, Batch 900, Loss: 0.096601134352386\n",
      "Epoch 5, Batch 100, Loss: 0.07332591880112886\n",
      "Epoch 5, Batch 200, Loss: 0.07857230645604432\n",
      "Epoch 5, Batch 300, Loss: 0.06562136488966644\n",
      "Epoch 5, Batch 400, Loss: 0.07493717172183097\n",
      "Epoch 5, Batch 500, Loss: 0.08084627166390419\n",
      "Epoch 5, Batch 600, Loss: 0.07783193414565176\n",
      "Epoch 5, Batch 700, Loss: 0.08883236736524851\n",
      "Epoch 5, Batch 800, Loss: 0.07929253947921097\n",
      "Epoch 5, Batch 900, Loss: 0.0801295191515237\n",
      "Finished Training\n",
      "Accuracy on test set: 96.70%\n",
      "Result: Accuracy = 96.70%\n",
      "--------------------------------------------------\n",
      "Experiment with: Optimizer=sgd, Loss=CrossEntropyLoss, Dropout=0.0, Activation=Sigmoid\n",
      "Epoch 1, Batch 100, Loss: 2.300927848815918\n",
      "Epoch 1, Batch 200, Loss: 2.2699933505058287\n",
      "Epoch 1, Batch 300, Loss: 2.1799897813796996\n",
      "Epoch 1, Batch 400, Loss: 1.9220127248764038\n",
      "Epoch 1, Batch 500, Loss: 1.4871585369110107\n",
      "Epoch 1, Batch 600, Loss: 1.1475647968053817\n",
      "Epoch 1, Batch 700, Loss: 0.9559310126304627\n",
      "Epoch 1, Batch 800, Loss: 0.8103115010261536\n",
      "Epoch 1, Batch 900, Loss: 0.7439283215999604\n",
      "Epoch 2, Batch 100, Loss: 0.6171255052089691\n",
      "Epoch 2, Batch 200, Loss: 0.5695255255699158\n",
      "Epoch 2, Batch 300, Loss: 0.5083749693632126\n",
      "Epoch 2, Batch 400, Loss: 0.46669867366552353\n",
      "Epoch 2, Batch 500, Loss: 0.4439320637285709\n",
      "Epoch 2, Batch 600, Loss: 0.4385043480992317\n",
      "Epoch 2, Batch 700, Loss: 0.4035269358754158\n",
      "Epoch 2, Batch 800, Loss: 0.38687617585062983\n",
      "Epoch 2, Batch 900, Loss: 0.3910949818789959\n",
      "Epoch 3, Batch 100, Loss: 0.35953350439667703\n",
      "Epoch 3, Batch 200, Loss: 0.3505475942790508\n",
      "Epoch 3, Batch 300, Loss: 0.35012197047472\n",
      "Epoch 3, Batch 400, Loss: 0.3432390238344669\n",
      "Epoch 3, Batch 500, Loss: 0.3564685924351215\n",
      "Epoch 3, Batch 600, Loss: 0.3257407981157303\n",
      "Epoch 3, Batch 700, Loss: 0.33744746550917626\n",
      "Epoch 3, Batch 800, Loss: 0.3080322605371475\n",
      "Epoch 3, Batch 900, Loss: 0.31756856605410577\n",
      "Epoch 4, Batch 100, Loss: 0.2911698966473341\n",
      "Epoch 4, Batch 200, Loss: 0.3028610149025917\n",
      "Epoch 4, Batch 300, Loss: 0.29663957498967647\n",
      "Epoch 4, Batch 400, Loss: 0.3050140193849802\n",
      "Epoch 4, Batch 500, Loss: 0.2809515731036663\n",
      "Epoch 4, Batch 600, Loss: 0.29291786640882495\n",
      "Epoch 4, Batch 700, Loss: 0.2876157832890749\n",
      "Epoch 4, Batch 800, Loss: 0.27757347032427787\n",
      "Epoch 4, Batch 900, Loss: 0.27176799774169924\n",
      "Epoch 5, Batch 100, Loss: 0.25450296826660634\n",
      "Epoch 5, Batch 200, Loss: 0.2732710115611553\n",
      "Epoch 5, Batch 300, Loss: 0.25864406809210777\n",
      "Epoch 5, Batch 400, Loss: 0.2683708329498768\n",
      "Epoch 5, Batch 500, Loss: 0.2526513106375933\n",
      "Epoch 5, Batch 600, Loss: 0.25363386575132607\n",
      "Epoch 5, Batch 700, Loss: 0.23761592626571656\n",
      "Epoch 5, Batch 800, Loss: 0.2524121903628111\n",
      "Epoch 5, Batch 900, Loss: 0.24144251123070717\n",
      "Finished Training\n",
      "Accuracy on test set: 92.77%\n",
      "Result: Accuracy = 92.77%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Experiment lists for hyperparameters\n",
    "hyperparameter_list_1 = {\n",
    "    'optimizers': ['rmsprop'],\n",
    "    'loss_functions': [nn.CrossEntropyLoss()],\n",
    "    'dropout_rates': [0.2],\n",
    "    'activation_functions': [nn.LeakyReLU, nn.Sigmoid]\n",
    "}\n",
    "\n",
    "hyperparameter_list_2 = {\n",
    "    'optimizers': ['sgd'],\n",
    "    'loss_functions': [nn.NLLLoss()],\n",
    "    'dropout_rates': [0.3],\n",
    "    'activation_functions': [nn.ReLU, nn.Tanh]\n",
    "}\n",
    "\n",
    "hyperparameter_list_3 = {\n",
    "    'optimizers': ['adam'],\n",
    "    'loss_functions': [nn.CrossEntropyLoss()],\n",
    "    'dropout_rates': [0.4],\n",
    "    'activation_functions': [nn.ReLU, nn.Sigmoid]\n",
    "}\n",
    "\n",
    "hyperparameter_list_4 = {\n",
    "    'optimizers': ['sgd'],\n",
    "    'loss_functions': [nn.CrossEntropyLoss()],\n",
    "    'dropout_rates': [0.0],\n",
    "    'activation_functions': [nn.ReLU, nn.Sigmoid]\n",
    "}\n",
    "# Run experiments with both hyperparameter lists\n",
    "print(\"Running experiments for Hyperparameter List 1:\")\n",
    "run_experiment(hyperparameter_list_1)\n",
    "\n",
    "print(\"\\nRunning experiments for Hyperparameter List 2:\")\n",
    "run_experiment(hyperparameter_list_2)\n",
    "\n",
    "print(\"\\nRunning experiments for Hyperparameter List 3:\")\n",
    "run_experiment(hyperparameter_list_3)\n",
    "\n",
    "print(\"\\nRunning experiments for Hyperparameter List 4:\")\n",
    "run_experiment(hyperparameter_list_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6cb631-53ae-4956-810b-de83d2420126",
   "metadata": {},
   "source": [
    "Based on the hyperparameters \n",
    "- SGD (no dropout, ReLU) has achieved the highest accuracy 96.70% on the test set. Without dropout and using ReLU seem to have positively impacted the performance.\n",
    "- Dropouts have generally led to slightly lower accuracy in most hyperparameters accuracy (between 95.79% and 96.34%). which tells that overfitting was not a major issue for this dataset, and the model has benefitted from keeping more connections active during training.\n",
    "- ReLU has consistently performed better across hyperparameter experiments by achieving accuracies above 96% in most cases.\n",
    "- Sigmoid activation has led to lower performance in general. The model trained with SGD and Sigmoid without dropout had the lowest accuracy (92.77%).\n",
    "- SGD and Adam performed comparably across different setups, with SGD slightly outperforming Adam in some cases.\n",
    "- RMSprop showed good performance, especially when paired with LeakyReLU, achieving 96.73% accuracy.\n",
    "- CrossEntropyLoss performed better than NLLLoss, likely due to its compatibility with the raw logits output by the model.\n",
    "\n",
    "Conclusion: The best performaning hyperparameter experiment was achieved using SGD with ReLU and no dropout (96.70%). This suggests that the model trained well without the need for dropout, and the use of ReLU helped optimize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077c2eb-aa5d-4376-b757-d3276571df4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
